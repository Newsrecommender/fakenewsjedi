{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "\n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n",
      "(422419, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import rcParams\n",
    "\n",
    "fake_df = pd.DataFrame.from_csv(\"fake.csv\")\n",
    "real_df = pd.DataFrame.from_csv(\"../uci-news-aggregator.csv\")\n",
    "\n",
    "fake_num_rows = fake_df.shape\n",
    "print(fake_num_rows)\n",
    "\n",
    "real_num_rows = real_df.shape\n",
    "print(real_num_rows)\n",
    "\n",
    "# df.head(100)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_df = real_df.head(12999)\n",
    "# print(real_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts_by_type = fake_df['type'].value_counts()\n",
    "print(counts_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['main_img_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(fake_df['spam_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['site_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_of_spam = fake_df['spam_score'].value_counts()\n",
    "# print(counts_of_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = fake_df.columns.tolist()\n",
    "# print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_replies = fake_df['replies_count'].value_counts()\n",
    "# print(counts_by_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_author = fake_df['author'].value_counts()\n",
    "# print(counts_by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_domain_rank = fake_df['domain_rank'].value_counts()\n",
    "# print(counts_by_domain_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "# \n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n"
     ]
    }
   ],
   "source": [
    "# create new \"id\" column in df \n",
    "# reorder column names, setting \"id\" as first column and delete \"uuid\" col \n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "fake_df = fake_df[['site_url', 'domain_rank', 'author', 'published', 'title', 'thread_title', 'text', 'ord_in_thread', 'crawled', 'country', 'language', 'spam_score', 'main_img_url', 'replies_count', 'participants_count', 'likes', 'comments', 'shares', 'type']]\n",
    "print(fake_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract text body from fake and save to file\n",
    "fake_text_only = fake_df[['text']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(fake_text_only) + 1):\n",
    "    text = fake_text_only.loc[i, 'text']\n",
    "    if type(text) != float:\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        text = text.split(\"\\t\")\n",
    "        text = \" \".join(text)\n",
    "        fake_text_only.set_value(i, 'text', text)\n",
    "\n",
    "fake_text_only.to_csv(\"fake_body_only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count ratio of number of exclamation marks to words in the given string\n",
    "def count_ratio_exclams(string):\n",
    "    exclam = '!'\n",
    "    space = \" \"\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_spaces = string.count(space)\n",
    "    if num_spaces == 0:\n",
    "        return num_exclams\n",
    "    else:\n",
    "        return num_exclams / num_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the ratio of exclams to question marks + periods in the given string\n",
    "def exclam_ratio_text_body(string):\n",
    "    exclam = '!'\n",
    "    period = '.'\n",
    "    question = '?'\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_period = string.count(period)\n",
    "    num_question = string.count(question)\n",
    "    if num_period + num_question == 0:\n",
    "        return num_exclams\n",
    "    return num_exclams / (num_period + num_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12987\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for ratio_exclam_in_title\n",
    "fake_df.assign(ratio_exclam_in_title=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "\n",
    "fake_df = fake_df[fake_df['thread_title'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    thread_title = fake_df.loc[i, 'thread_title']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "# counts_by_title_exclams = fake_df.total_exclam_in_title.value_counts()\n",
    "# print(counts_by_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12941\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for total_exclam_in_text count\n",
    "fake_df.assign(ratio_exclam_in_text_body=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "fake_df = fake_df[fake_df['text'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_text_body', count)\n",
    "\n",
    "ratio_in_text_body = fake_df.ratio_exclam_in_text_body.value_counts()\n",
    "# print(ratio_in_text_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclams_in_text count\n",
    "fake_df.assign(ratio_exclams_in_text=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclams_in_text\"\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclams_in_text_body', count)\n",
    "\n",
    "#counts_ratio_exclams = fake_df.ratio_exclams_in_text.value_counts()\n",
    "# print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000    12670\n",
      "0.125000       41\n",
      "0.111111       38\n",
      "0.142857       36\n",
      "0.090909       35\n",
      "0.100000       31\n",
      "0.166667       29\n",
      "0.200000       25\n",
      "0.076923       22\n",
      "0.083333       20\n",
      "0.071429       17\n",
      "0.066667       11\n",
      "0.250000        9\n",
      "0.153846        3\n",
      "0.333333        3\n",
      "0.222222        2\n",
      "0.058824        2\n",
      "0.285714        2\n",
      "0.500000        1\n",
      "0.300000        1\n",
      "0.052632        1\n",
      "Name: ratio_exclam_in_title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for ratio_exclam_in_title count\n",
    "real_df.assign(ratio_exclam_in_title=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclam_in_title\"\n",
    "for i in range(1, len(real_df) + 1):\n",
    "    thread_title = real_df.loc[i, 'TITLE']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    real_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "counts_ratio_exclams = real_df.ratio_exclam_in_title.value_counts()\n",
    "print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create series of total exclamation counts in each row's title\n",
    "# for index, row in df.iterrows():\n",
    "#     count = count_total_exclamation(row.title)\n",
    "#     print(count)\n",
    "#     df.loc[:,'total_crime'] = df.apply(get_total_crime, axis=1)\n",
    "#     df.loc[index, row.total_exclam_in_title] = count\n",
    "\n",
    "\n",
    "# df.loc[:, 'total_exclam_in_title'] = df.apply(count_total_exclams, axis=1)    \n",
    "# count_title_exclams = df['total_exclam_in_title'].value_counts()\n",
    "# print(count_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make  copy of the fake_df containing only the thread_title & site_url\n",
    "sub_fake_df = fake_df[['thread_title', 'site_url', 'ratio_exclam_in_title']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    title = sub_fake_df.loc[i, 'thread_title']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_fake_df.set_value(i, 'thread_title', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    url = sub_fake_df.loc[i, 'site_url']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_fake_df.set_value(i, 'site_url', url)\n",
    "\n",
    "# turn all tabs into spaces\n",
    "# x = \"The bananas are yellow and green\"\n",
    "# x = x.split(\" \")\n",
    "# print(x)\n",
    "# x = \"+\".join(x)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_real_df = real_df[['TITLE', 'URL', 'ratio_exclam_in_title']].copy()\n",
    "sub_real_df = sub_real_df.head(12941)\n",
    "\n",
    "# correct id labels\n",
    "sub_real_df['id'] = range(1, len(sub_real_df) + 1)\n",
    "sub_real_df = sub_real_df.set_index('id')\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    title = sub_real_df.loc[i, 'TITLE']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_real_df.set_value(i, 'TITLE', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    url = sub_real_df.loc[i, 'URL']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_real_df.set_value(i, 'URL', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   TITLE  \\\n",
      "id                                                         \n",
      "1      Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
      "2      Re: Why Did Attorney General Loretta Lynch Ple...   \n",
      "3      BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
      "4      PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
      "5      FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
      "6      Hillary Goes Absolutely Berserk On Protester A...   \n",
      "7      BREAKING! NYPD Ready To Make Arrests In Weiner...   \n",
      "8      WOW! WHISTLEBLOWER TELLS CHILLING STORY Of Mas...   \n",
      "9      BREAKING: CLINTON CLEARED...Was This A Coordin...   \n",
      "10     EVIL HILLARY SUPPORTERS Yell \"F*ck Trump\"…Burn...   \n",
      "11     YIKES! HILLARY GOES OFF THE RAILS…Pulls A Howa...   \n",
      "12     SAY GOODBYE! These 23 Hollywood Celebs Threate...   \n",
      "13     NOT KIDDING: Colleges Give Students “Safe Spac...   \n",
      "14     BOOM! MATH SHOWS Trump Would Have Beaten Obama...   \n",
      "15     BOOM! This Is How President Reagan Handled Pro...   \n",
      "16     TRUMP SUPPORTER GOT NUTS On MSNBC Reporter Cov...   \n",
      "17     TOMI LAHREN Has Special Message For Celebritie...   \n",
      "18     #BoycottComedian…ROBERT DENIRO Wanted “To Punc...   \n",
      "19     HE’S NEVER SOLD AN ORIGINAL PAINTING UNTIL NOW...   \n",
      "20     SORRY LIBERALS…You Can Stop With The Petitions...   \n",
      "21     MARK CUBAN: \"In The Event Donald Wins, I Have ...   \n",
      "22     TRUMP SUPPORTER Whose Brutal Beating By Black ...   \n",
      "23     WOW! WHITE Liberals Suggest Blacks Are Too Stu...   \n",
      "24     LOL! BRITISH WIFE Of LIB ACTOR Who Said: “Ther...   \n",
      "25     EPIC! TUCKER CARLSON Demolishes NYC Councilman...   \n",
      "26     FUNNY! SNL’S SOLUTION To Democrat Election Den...   \n",
      "27     DONALD TRUMP Calls Meeting With Press…Dresses ...   \n",
      "28     OOPS! CRYBABY HAMILTON STARS Who Lectured Penc...   \n",
      "29     WHITE TRUMP-BASHING LIB Reprimanded On LIVE CN...   \n",
      "30         BOOM! Kellyanne Conway Shuts Down CNN’s Cuomo   \n",
      "...                                                  ...   \n",
      "25853  US Navy SEALs board tanker hijacked in Libya -...   \n",
      "25854  UPDATE 2-US forces seize tanker carrying oil f...   \n",
      "25855  Norfolk-based destroyer to aid tanker seized b...   \n",
      "25856         US Navy SEALs board rogue Libya oil tanker   \n",
      "25857          U.S. forces seize control of rogue tanker   \n",
      "25858            US Navy SEALs seize fugitive oil tanker   \n",
      "25859  US Navy SEALs board tanker carrying oil from L...   \n",
      "25860  Shades of 'Captain Phillips': Navy SEALS retak...   \n",
      "25861      US Seals take control of rogue Libya oil ship   \n",
      "25862  US Navy SEALs seize stolen Libyan oil tanker h...   \n",
      "25863  Navy SEALs board and take control of oil tanke...   \n",
      "25864  US Special forces take control of rogue Libya ...   \n",
      "25865        Navy SEALs seize control of hijacked tanker   \n",
      "25866  US forces seize tanker carrying oil from Libya...   \n",
      "25867  US Navy Seals take control of rogue Libya oil ...   \n",
      "25868  US Seals take control of rogue Libya oil ship:...   \n",
      "25869  Navy SEALs board mystery tanker Morning Glory ...   \n",
      "25870  US Navy Seals retake hijacked oil tanker off t...   \n",
      "25871   Cyprus: SEALs take oil tanker from Libyan rebels   \n",
      "25872        US Navy Seals seize North Korean oil tanker   \n",
      "25873            Navy Seals board rogue Libya oil tanker   \n",
      "25874  US Navy Seal Commandos Seize North Korea Oil T...   \n",
      "25875  Israeli pair questioned over Pyongyang-bound o...   \n",
      "25876  Navy SEALs Board Oil Tanker Stolen From Libyan...   \n",
      "25877  US Seals take control of rogue Libya ship: Pen...   \n",
      "25878     Navy SEALS take control of hijacked oil tanker   \n",
      "25879  Navy SEALs take control of hijacked Libyan oil...   \n",
      "25880  U.S. Navy SEALs take control of North Korean-f...   \n",
      "25881  Oil tanker heading back to Libya after capture...   \n",
      "25882             US Seals storm 'oil theft' Libyan ship   \n",
      "\n",
      "                                                     URL  \\\n",
      "id                                                         \n",
      "1                                    100percentfedup.com   \n",
      "2                                    100percentfedup.com   \n",
      "3                                    100percentfedup.com   \n",
      "4                                    100percentfedup.com   \n",
      "5                                    100percentfedup.com   \n",
      "6                                    100percentfedup.com   \n",
      "7                                    100percentfedup.com   \n",
      "8                                    100percentfedup.com   \n",
      "9                                    100percentfedup.com   \n",
      "10                                   100percentfedup.com   \n",
      "11                                   100percentfedup.com   \n",
      "12                                   100percentfedup.com   \n",
      "13                                   100percentfedup.com   \n",
      "14                                   100percentfedup.com   \n",
      "15                                   100percentfedup.com   \n",
      "16                                   100percentfedup.com   \n",
      "17                                   100percentfedup.com   \n",
      "18                                   100percentfedup.com   \n",
      "19                                   100percentfedup.com   \n",
      "20                                   100percentfedup.com   \n",
      "21                                   100percentfedup.com   \n",
      "22                                   100percentfedup.com   \n",
      "23                                   100percentfedup.com   \n",
      "24                                   100percentfedup.com   \n",
      "25                                   100percentfedup.com   \n",
      "26                                   100percentfedup.com   \n",
      "27                                   100percentfedup.com   \n",
      "28                                   100percentfedup.com   \n",
      "29                                   100percentfedup.com   \n",
      "30                                   100percentfedup.com   \n",
      "...                                                  ...   \n",
      "25853  http://www.reuters.com/article/2014/03/17/usa-...   \n",
      "25854  http://www.reuters.com/article/2014/03/17/usa-...   \n",
      "25855  http://hamptonroads.com/2014/03/norfolkbased-d...   \n",
      "25856  http://www.aljazeera.com/news/africa/2014/03/u...   \n",
      "25857  http://www.ksdk.com/story/news/nation/2014/03/...   \n",
      "25858  http://www.latimes.com/world/worldnow/la-fg-wn...   \n",
      "25859  http://uk.reuters.com/article/2014/03/17/uk-us...   \n",
      "25860  http://www.arktimes.com/ArkansasBlog/archives/...   \n",
      "25861  http://www.ptinews.com/news/4509295_-US-Seals-...   \n",
      "25862  http://www.allvoices.com/contributed-news/1671...   \n",
      "25863  http://www.newser.com/article/a119b55666b54365...   \n",
      "25864  http://www.brisbanetimes.com.au/world/us-speci...   \n",
      "25865  http://www.abc17news.com/national-news/Navy-SE...   \n",
      "25866  http://www.worldbulletin.net/world/131244/us-f...   \n",
      "25867  http://news.yahoo.com/us-seals-control-rogue-l...   \n",
      "25868  http://www.ptinews.com/news/4509295_US-Seals-t...   \n",
      "25869  http://www.washingtonpost.com/news/morning-mix...   \n",
      "25870  http://www.rawstory.com/rs/2014/03/17/us-navy-...   \n",
      "25871  http://www.politico.com/story/2014/03/cyprus-n...   \n",
      "25872  http://voiceofrussia.com/news/2014_03_17/US-Na...   \n",
      "25873  http://blouinnews.com/78329/story/navy-seals-b...   \n",
      "25874  http://www.ibtimes.co.uk/us-navy-seal-commando...   \n",
      "25875  http://www.thestandard.com.hk/breaking_news_de...   \n",
      "25876  http://www.huffingtonpost.com/2014/03/17/navy-...   \n",
      "25877  http://www.ptinews.com/news/4509045_US-Seals-t...   \n",
      "25878  http://www.washingtontimes.com/news/2014/mar/1...   \n",
      "25879  http://wtkr.com/2014/03/17/navy-seals-take-con...   \n",
      "25880  http://www.dailymail.co.uk/news/article-258257...   \n",
      "25881  http://www.libyaherald.com/2014/03/17/oil-tank...   \n",
      "25882  http://www.upstreamonline.com/live/article1355...   \n",
      "\n",
      "       ratio_exclam_in_title  TARGET  \n",
      "id                                    \n",
      "1                   0.000000       1  \n",
      "2                   0.000000       1  \n",
      "3                   0.000000       1  \n",
      "4                   0.050000       1  \n",
      "5                   0.153846       1  \n",
      "6                   0.125000       1  \n",
      "7                   0.038462       1  \n",
      "8                   0.043478       1  \n",
      "9                   0.000000       1  \n",
      "10                  0.000000       1  \n",
      "11                  0.111111       1  \n",
      "12                  0.125000       1  \n",
      "13                  0.000000       1  \n",
      "14                  0.083333       1  \n",
      "15                  0.062500       1  \n",
      "16                  0.000000       1  \n",
      "17                  0.000000       1  \n",
      "18                  0.000000       1  \n",
      "19                  0.000000       1  \n",
      "20                  0.000000       1  \n",
      "21                  0.000000       1  \n",
      "22                  0.000000       1  \n",
      "23                  0.058824       1  \n",
      "24                  0.050000       1  \n",
      "25                  0.111111       1  \n",
      "26                  0.090909       1  \n",
      "27                  0.000000       1  \n",
      "28                  0.076923       1  \n",
      "29                  0.000000       1  \n",
      "30                  0.166667       1  \n",
      "...                      ...     ...  \n",
      "25853               0.000000       0  \n",
      "25854               0.000000       0  \n",
      "25855               0.000000       0  \n",
      "25856               0.000000       0  \n",
      "25857               0.000000       0  \n",
      "25858               0.000000       0  \n",
      "25859               0.000000       0  \n",
      "25860               0.000000       0  \n",
      "25861               0.000000       0  \n",
      "25862               0.000000       0  \n",
      "25863               0.000000       0  \n",
      "25864               0.000000       0  \n",
      "25865               0.000000       0  \n",
      "25866               0.000000       0  \n",
      "25867               0.000000       0  \n",
      "25868               0.000000       0  \n",
      "25869               0.000000       0  \n",
      "25870               0.000000       0  \n",
      "25871               0.000000       0  \n",
      "25872               0.000000       0  \n",
      "25873               0.000000       0  \n",
      "25874               0.000000       0  \n",
      "25875               0.000000       0  \n",
      "25876               0.000000       0  \n",
      "25877               0.000000       0  \n",
      "25878               0.000000       0  \n",
      "25879               0.000000       0  \n",
      "25880               0.000000       0  \n",
      "25881               0.000000       0  \n",
      "25882               0.000000       0  \n",
      "\n",
      "[25882 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# create new column, 'TARGET' with 1 fake and 0 for real\n",
    "sub_fake_df['TARGET'] = 1\n",
    "sub_fake_df = sub_fake_df.rename(columns = {'thread_title':'TITLE', 'site_url':'URL'})\n",
    "sub_real_df['TARGET'] = 0\n",
    "\n",
    "# combine the two dataframes\n",
    "combined_df = sub_fake_df.append(sub_real_df)\n",
    "\n",
    "# reorder the id index of the combined_df set\n",
    "# correct id labels\n",
    "combined_df['id'] = range(1, len(combined_df) + 1)\n",
    "combined_df = combined_df.set_index('id')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "combined_df = shuffle(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert combined_df into a new TAB DELIMITED csv\n",
    "combined_df.to_csv(\"cleaned_combined_dataset.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(combined_df)) < 0.75\n",
    "training_set = combined_df[sampler]\n",
    "test_set = combined_df[~sampler]\n",
    "\n",
    "#print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set.to_csv(\"training_set.csv\", sep='\\t', index=False)\n",
    "test_set.to_csv(\"holdout_set.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stem the \"fake news\" data\n",
    "ps = PorterStemmer()\n",
    "fake_blob = {} \n",
    "real_blob = {}\n",
    "for i in range(len(training_set['TARGET'])):\n",
    "    try:\n",
    "        ss8 = str(training_set['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    # if this is a 'fake' row entry\n",
    "    if training_set['TARGET'].iloc[i] == 1:\n",
    "        for stword in x:\n",
    "            if stword in fake_blob:\n",
    "                fake_blob[stword] = fake_blob[stword] + 1\n",
    "                #print(stword, \" \", fake_blob[stword])\n",
    "            else:\n",
    "                fake_blob.setdefault(stword, 1)\n",
    "                #print(stword,\" \", fake_blob[stword])\n",
    "                \n",
    "    # we found a 'real' row entry\n",
    "    else:\n",
    "        for stword in x:\n",
    "            if stword in real_blob:\n",
    "                real_blob[stword] = real_blob[stword] + 1\n",
    "            else:\n",
    "                real_blob.setdefault(stword, 1)\n",
    "# print(fakeBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # stem the \"real news\" data    \n",
    "# goodBlob = {}\n",
    "# for i in range(len(sub_real_df['TITLE'])):\n",
    "#     try:\n",
    "#         ss8 = str(sub_real_df['TITLE'].iloc[i].encode('utf8'))\n",
    "#     except:\n",
    "#         pass\n",
    "#     words = word_tokenize(ss8)\n",
    "#     x = set()\n",
    "#     for w in words:\n",
    "#         x.add(ps.stem(w).lower())\n",
    "\n",
    "#     for stword in x:\n",
    "#         if stword in goodBlob:\n",
    "#             goodBlob[stword] = goodBlob[stword] + 1\n",
    "#             #print(stword, \" \", goodBlob[stword])\n",
    "#         else:\n",
    "#             goodBlob.setdefault(stword,1)\n",
    "#             #print(stword,\" \", goodBlob[stword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskdict = {}\n",
    "for word in fake_blob:\n",
    "    if word in real_blob:\n",
    "        count = (fake_blob[word] + real_blob[word])\n",
    "    else:\n",
    "        count = fake_blob[word]\n",
    "    if count >= 10:\n",
    "        riskdict[word] = fake_blob[word] / count\n",
    "\n",
    "for word in real_blob:\n",
    "    if word not in fake_blob and real_blob[word] >= 10:\n",
    "        riskdict[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TITLE', 'URL', 'ratio_exclam_in_title', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']\n"
     ]
    }
   ],
   "source": [
    "# add four columns to the results_df: fakeaggregate, goodaggregate, riskword, safeword\n",
    "results_df = combined_df.copy()\n",
    "results_df['fake_aggregate'] = 0\n",
    "results_df['good_aggregate'] = 0\n",
    "results_df['risk_word'] = 0\n",
    "results_df['safe_word'] = 0\n",
    "\n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-79e6ecbc7575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'safe_word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafeword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mriskword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "riskword = \"\"\n",
    "for i in range(len(results_df['TITLE'])):\n",
    "    fakeaggregate = 0\n",
    "    goodaggregate = 0\n",
    "    riskyword = 0\n",
    "    safeword = 1\n",
    "    try:\n",
    "        ss8 = str(results_df['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    for stword in x:\n",
    "        if stword in riskdict:\n",
    "            if riskdict[stword] > riskyword:\n",
    "                riskword = riskdict[stword]\n",
    "            if riskdict[stword] < safeword:\n",
    "                safeword = riskdict[stword]\n",
    "\n",
    "        if stword in fake_blob:\n",
    "            fakeaggregate = fake_blob[stword] + fakeaggregate\n",
    "\n",
    "        if stword in real_blob:\n",
    "            goodaggregate = real_blob[stword] + goodaggregate\n",
    "    # update the results to results_df, training_df, test_df        \n",
    "    results_df.set_value(i, 'fake_aggregate', fakeaggregate)\n",
    "    results_df.set_value(i, 'good_aggregate', goodaggregate)\n",
    "    results_df.set_value(i, 'risk_word', riskword)\n",
    "    results_df.set_value(i, 'safe_word', safeword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2792\n",
      "1.0\n",
      "321\n",
      "[(92, 'muslim'), (94, 'trump\\\\xe2\\\\x80\\\\x99'), (99, 'de'), (101, 'isi'), (103, 'wikileak'), (105, \"b're\"), (117, 'syria'), (151, \"b'hillari\"), (156, '2016'), (194, 'donald'), (210, \"b'trump\"), (247, 'fbi'), (273, 'email'), (676, 'hillari')]\n"
     ]
    }
   ],
   "source": [
    "print(len(riskdict))\n",
    "print(max(riskdict.values()))\n",
    "\n",
    "print(len([x for x in riskdict.keys() if riskdict[x] == 1]))\n",
    "y = [x for x in riskdict.keys() if riskdict[x] == 1]\n",
    "z = {x:fake_blob[x] for x in y}\n",
    "\n",
    "a = sorted(z.values())\n",
    "print(sorted([(z[x],x) for x in z if z[x] >= 90]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TARGET  ratio_exclam_in_title  fake_aggregate  good_aggregate  \\\n",
      "id                                                                     \n",
      "22972     0.0               0.000000         15307.0         13353.0   \n",
      "24184     0.0               0.000000         17645.0         13501.0   \n",
      "2319      1.0               0.000000         14885.0         11657.0   \n",
      "22839     0.0               0.000000         10856.0          8833.0   \n",
      "4095      1.0               0.000000          5009.0         10715.0   \n",
      "15820     0.0               0.066667          9360.0          8328.0   \n",
      "7721      1.0               0.000000         13871.0         17851.0   \n",
      "14087     0.0               0.000000         10198.0          8975.0   \n",
      "6715      1.0               0.000000         14796.0         11371.0   \n",
      "22415     0.0               0.000000          9988.0          8807.0   \n",
      "14891     0.0               0.000000         18944.0         14281.0   \n",
      "25675     0.0               0.000000          7901.0         10922.0   \n",
      "22081     0.0               0.000000         10947.0          9370.0   \n",
      "946       1.0               0.000000         11132.0         10309.0   \n",
      "8509      1.0               0.000000         12144.0         10604.0   \n",
      "617       1.0               0.000000         10929.0         15019.0   \n",
      "13231     0.0               0.000000         10761.0          9557.0   \n",
      "10287     1.0               0.000000          9475.0          8082.0   \n",
      "10787     1.0               0.000000         13326.0         10266.0   \n",
      "19918     0.0               0.000000         17430.0         13857.0   \n",
      "19848     0.0               0.000000         12281.0         11123.0   \n",
      "21137     0.0               0.000000         13235.0         11079.0   \n",
      "5741      1.0               0.000000         14403.0         11608.0   \n",
      "18598     0.0               0.000000         13325.0         11077.0   \n",
      "3501      1.0               0.000000         13531.0         10020.0   \n",
      "14723     0.0               0.000000          4577.0          9676.0   \n",
      "18919     0.0               0.000000         13665.0         10776.0   \n",
      "9938      1.0               0.000000         16825.0         13572.0   \n",
      "7804      1.0               0.000000          3697.0         10182.0   \n",
      "3005      1.0               0.000000         12607.0         10867.0   \n",
      "...       ...                    ...             ...             ...   \n",
      "19332     0.0               0.000000          7374.0         13623.0   \n",
      "7166      1.0               0.000000         10083.0         12844.0   \n",
      "9163      1.0               0.000000         13674.0         13133.0   \n",
      "17379     0.0               0.000000         15827.0         12335.0   \n",
      "24123     0.0               0.000000         18158.0         14114.0   \n",
      "1905      1.0               0.000000         10057.0          8575.0   \n",
      "9312      1.0               0.000000         10553.0         10024.0   \n",
      "1291      1.0               0.000000         10247.0          9187.0   \n",
      "589       1.0               0.000000         12862.0         10776.0   \n",
      "12253     1.0               0.000000          9880.0          9103.0   \n",
      "5070      1.0               0.000000         10991.0         15217.0   \n",
      "20147     0.0               0.000000         12128.0         10424.0   \n",
      "1234      1.0               0.000000          2627.0          9161.0   \n",
      "499       1.0               0.000000         11468.0         16190.0   \n",
      "8121      1.0               0.000000         12339.0         10721.0   \n",
      "15623     0.0               0.000000         12761.0         10123.0   \n",
      "2913      1.0               0.000000         16895.0         13285.0   \n",
      "24305     0.0               0.000000         10238.0          8834.0   \n",
      "12142     1.0               0.000000         14499.0         12026.0   \n",
      "22790     0.0               0.000000         13733.0         11369.0   \n",
      "23056     0.0               0.000000         10890.0         10132.0   \n",
      "11366     1.0               0.000000         11431.0          9333.0   \n",
      "18486     0.0               0.000000         12673.0         10524.0   \n",
      "2302      1.0               0.090909         12256.0         10407.0   \n",
      "3110      1.0               0.000000         13668.0         10495.0   \n",
      "699       1.0               0.000000         14654.0         12800.0   \n",
      "19987     0.0               0.000000         11684.0         10080.0   \n",
      "6115      1.0               0.000000         14642.0         14517.0   \n",
      "18396     0.0               0.000000          9341.0          8129.0   \n",
      "0         NaN                    NaN         13793.0         12641.0   \n",
      "\n",
      "       risk_word  safe_word  \n",
      "id                           \n",
      "22972   0.383333   0.383333  \n",
      "24184   0.998004   0.517889  \n",
      "2319    0.761905   0.533794  \n",
      "22839   0.711538   0.533794  \n",
      "4095    1.000000   0.188670  \n",
      "15820   0.625000   0.000000  \n",
      "7721    0.359375   0.000000  \n",
      "14087   0.600462   0.105263  \n",
      "6715    0.218750   0.218750  \n",
      "22415   0.528302   0.000000  \n",
      "14891   0.622291   0.365079  \n",
      "25675   0.616631   0.188670  \n",
      "22081   0.538760   0.533794  \n",
      "946     0.538760   0.000000  \n",
      "8509    0.500000   0.454545  \n",
      "617     0.342857   0.000000  \n",
      "13231   0.678571   0.303030  \n",
      "10287   1.000000   0.066667  \n",
      "10787   0.908602   0.533794  \n",
      "19918   0.538760   0.454545  \n",
      "19848   0.222222   0.000000  \n",
      "21137   0.354839   0.000000  \n",
      "5741    0.375000   0.218750  \n",
      "18598   0.622291   0.483871  \n",
      "3501    0.780952   0.533794  \n",
      "14723   0.616631   0.027778  \n",
      "18919   0.666667   0.241071  \n",
      "9938    0.594595   0.000000  \n",
      "7804    0.414414   0.000000  \n",
      "3005    0.616631   0.362319  \n",
      "...          ...        ...  \n",
      "19332   0.303030   0.000000  \n",
      "7166    0.876404   0.188670  \n",
      "9163    0.126214   0.000000  \n",
      "17379   0.900000   0.533794  \n",
      "24123   0.666667   0.368664  \n",
      "1905    0.500000   0.352941  \n",
      "9312    0.426117   0.037975  \n",
      "1291    0.622291   0.000000  \n",
      "589     0.630252   0.328173  \n",
      "12253   0.126214   0.003236  \n",
      "5070    0.040000   0.037975  \n",
      "20147   0.943020   0.000000  \n",
      "1234    0.211106   0.000000  \n",
      "499     0.600462   0.000000  \n",
      "8121    0.573840   0.050000  \n",
      "15623   0.126214   0.126214  \n",
      "2913    0.809524   0.365591  \n",
      "24305   0.714286   0.032258  \n",
      "12142   0.777778   0.109091  \n",
      "22790   0.577778   0.482558  \n",
      "23056   0.080569   0.080569  \n",
      "11366   0.616631   0.272727  \n",
      "18486   0.618557   0.450704  \n",
      "2302    1.000000   0.380000  \n",
      "3110    0.616631   0.460317  \n",
      "699     0.538760   0.232323  \n",
      "19987   0.733333   0.458401  \n",
      "6115    0.243243   0.038828  \n",
      "18396   0.564516   0.338983  \n",
      "0       0.409467   0.000000  \n",
      "\n",
      "[19454 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(results_df)) < 0.75\n",
    "new_training = results_df[sampler]\n",
    "new_test = results_df[~sampler]\n",
    "\n",
    "# new_training = new_training[['TITLE', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "new_training = new_training[['TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "print(new_training)\n",
    "\n",
    "holdout_title_features = new_test[['TITLE', 'TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "\n",
    "new_training.to_csv(\"new_training.csv\", sep='\\t', index=False)\n",
    "new_test.to_csv(\"new_holdout.csv\", sep='\\t', index=False)\n",
    "\n",
    "holdout_title_features.to_csv(\"holdout_title_features.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnoff Test:\n",
    "\n",
    "prediction_df = pd.DataFrame.from_csv(\"good_model_prediction_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
