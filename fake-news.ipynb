{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "\n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n",
      "(422419, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import rcParams\n",
    "\n",
    "fake_df = pd.DataFrame.from_csv(\"fake.csv\")\n",
    "real_df = pd.DataFrame.from_csv(\"../uci-news-aggregator.csv\")\n",
    "\n",
    "fake_num_rows = fake_df.shape\n",
    "print(fake_num_rows)\n",
    "\n",
    "real_num_rows = real_df.shape\n",
    "print(real_num_rows)\n",
    "\n",
    "# df.head(100)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_df = real_df.head(12999)\n",
    "# print(real_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts_by_type = fake_df['type'].value_counts()\n",
    "print(counts_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['main_img_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(fake_df['spam_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['site_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_of_spam = fake_df['spam_score'].value_counts()\n",
    "# print(counts_of_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = fake_df.columns.tolist()\n",
    "# print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_replies = fake_df['replies_count'].value_counts()\n",
    "# print(counts_by_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_author = fake_df['author'].value_counts()\n",
    "# print(counts_by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_domain_rank = fake_df['domain_rank'].value_counts()\n",
    "# print(counts_by_domain_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "# \n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n"
     ]
    }
   ],
   "source": [
    "# create new \"id\" column in df \n",
    "# reorder column names, setting \"id\" as first column and delete \"uuid\" col \n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "fake_df = fake_df[['site_url', 'domain_rank', 'author', 'published', 'title', 'thread_title', 'text', 'ord_in_thread', 'crawled', 'country', 'language', 'spam_score', 'main_img_url', 'replies_count', 'participants_count', 'likes', 'comments', 'shares', 'type']]\n",
    "print(fake_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count ratio of number of exclamation marks to words in the given string\n",
    "def count_ratio_exclams(string):\n",
    "    exclam = '!'\n",
    "    space = \" \"\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_spaces = string.count(space)\n",
    "    if num_spaces == 0:\n",
    "        return num_exclams\n",
    "    else:\n",
    "        return num_exclams / num_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the ratio of exclams to question marks + periods in the given string\n",
    "def exclam_ratio_text_body(string):\n",
    "    exclam = '!'\n",
    "    period = '.'\n",
    "    question = '?'\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_period = string.count(period)\n",
    "    num_question = string.count(question)\n",
    "    if num_period + num_question == 0:\n",
    "        return num_exclams\n",
    "    return num_exclams / (num_period + num_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12987\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for ratio_exclam_in_title\n",
    "fake_df.assign(ratio_exclam_in_title=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "\n",
    "fake_df = fake_df[fake_df['thread_title'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    thread_title = fake_df.loc[i, 'thread_title']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "# counts_by_title_exclams = fake_df.total_exclam_in_title.value_counts()\n",
    "# print(counts_by_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12941\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for total_exclam_in_text count\n",
    "fake_df.assign(ratio_exclam_in_text_body=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "fake_df = fake_df[fake_df['text'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_text_body', count)\n",
    "\n",
    "ratio_in_text_body = fake_df.ratio_exclam_in_text_body.value_counts()\n",
    "# print(ratio_in_text_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclams_in_text count\n",
    "fake_df.assign(ratio_exclams_in_text=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclams_in_text\"\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclams_in_text_body', count)\n",
    "\n",
    "counts_ratio_exclams = fake_df.ratio_exclams_in_text.value_counts()\n",
    "# print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000    12670\n",
      "0.125000       41\n",
      "0.111111       38\n",
      "0.142857       36\n",
      "0.090909       35\n",
      "0.100000       31\n",
      "0.166667       29\n",
      "0.200000       25\n",
      "0.076923       22\n",
      "0.083333       20\n",
      "0.071429       17\n",
      "0.066667       11\n",
      "0.250000        9\n",
      "0.153846        3\n",
      "0.333333        3\n",
      "0.222222        2\n",
      "0.058824        2\n",
      "0.285714        2\n",
      "0.500000        1\n",
      "0.300000        1\n",
      "0.052632        1\n",
      "Name: ratio_exclam_in_title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create new empty column for ratio_exclam_in_title count\n",
    "real_df.assign(ratio_exclam_in_title=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclam_in_title\"\n",
    "for i in range(1, len(real_df) + 1):\n",
    "    thread_title = real_df.loc[i, 'TITLE']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    real_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "counts_ratio_exclams = real_df.ratio_exclam_in_title.value_counts()\n",
    "print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create series of total exclamation counts in each row's title\n",
    "# for index, row in df.iterrows():\n",
    "#     count = count_total_exclamation(row.title)\n",
    "#     print(count)\n",
    "#     df.loc[:,'total_crime'] = df.apply(get_total_crime, axis=1)\n",
    "#     df.loc[index, row.total_exclam_in_title] = count\n",
    "\n",
    "\n",
    "# df.loc[:, 'total_exclam_in_title'] = df.apply(count_total_exclams, axis=1)    \n",
    "# count_title_exclams = df['total_exclam_in_title'].value_counts()\n",
    "# print(count_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make  copy of the fake_df containing only the thread_title & site_url\n",
    "sub_fake_df = fake_df[['thread_title', 'site_url', 'ratio_exclam_in_title']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    title = sub_fake_df.loc[i, 'thread_title']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_fake_df.set_value(i, 'thread_title', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    url = sub_fake_df.loc[i, 'site_url']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_fake_df.set_value(i, 'site_url', url)\n",
    "\n",
    "# turn all tabs into spaces\n",
    "# x = \"The bananas are yellow and green\"\n",
    "# x = x.split(\" \")\n",
    "# print(x)\n",
    "# x = \"+\".join(x)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_real_df = real_df[['TITLE', 'URL', 'ratio_exclam_in_title']].copy()\n",
    "sub_real_df = sub_real_df.head(12941)\n",
    "\n",
    "# correct id labels\n",
    "sub_real_df['id'] = range(1, len(sub_real_df) + 1)\n",
    "sub_real_df = sub_real_df.set_index('id')\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    title = sub_real_df.loc[i, 'TITLE']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_real_df.set_value(i, 'TITLE', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    url = sub_real_df.loc[i, 'URL']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_real_df.set_value(i, 'URL', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   TITLE  \\\n",
      "id                                                         \n",
      "1      Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
      "2      Re: Why Did Attorney General Loretta Lynch Ple...   \n",
      "3      BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
      "4      PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
      "5      FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
      "6      Hillary Goes Absolutely Berserk On Protester A...   \n",
      "7      BREAKING! NYPD Ready To Make Arrests In Weiner...   \n",
      "8      WOW! WHISTLEBLOWER TELLS CHILLING STORY Of Mas...   \n",
      "9      BREAKING: CLINTON CLEARED...Was This A Coordin...   \n",
      "10     EVIL HILLARY SUPPORTERS Yell \"F*ck Trump\"…Burn...   \n",
      "11     YIKES! HILLARY GOES OFF THE RAILS…Pulls A Howa...   \n",
      "12     SAY GOODBYE! These 23 Hollywood Celebs Threate...   \n",
      "13     NOT KIDDING: Colleges Give Students “Safe Spac...   \n",
      "14     BOOM! MATH SHOWS Trump Would Have Beaten Obama...   \n",
      "15     BOOM! This Is How President Reagan Handled Pro...   \n",
      "16     TRUMP SUPPORTER GOT NUTS On MSNBC Reporter Cov...   \n",
      "17     TOMI LAHREN Has Special Message For Celebritie...   \n",
      "18     #BoycottComedian…ROBERT DENIRO Wanted “To Punc...   \n",
      "19     HE’S NEVER SOLD AN ORIGINAL PAINTING UNTIL NOW...   \n",
      "20     SORRY LIBERALS…You Can Stop With The Petitions...   \n",
      "21     MARK CUBAN: \"In The Event Donald Wins, I Have ...   \n",
      "22     TRUMP SUPPORTER Whose Brutal Beating By Black ...   \n",
      "23     WOW! WHITE Liberals Suggest Blacks Are Too Stu...   \n",
      "24     LOL! BRITISH WIFE Of LIB ACTOR Who Said: “Ther...   \n",
      "25     EPIC! TUCKER CARLSON Demolishes NYC Councilman...   \n",
      "26     FUNNY! SNL’S SOLUTION To Democrat Election Den...   \n",
      "27     DONALD TRUMP Calls Meeting With Press…Dresses ...   \n",
      "28     OOPS! CRYBABY HAMILTON STARS Who Lectured Penc...   \n",
      "29     WHITE TRUMP-BASHING LIB Reprimanded On LIVE CN...   \n",
      "30         BOOM! Kellyanne Conway Shuts Down CNN’s Cuomo   \n",
      "...                                                  ...   \n",
      "25853  US Navy SEALs board tanker hijacked in Libya -...   \n",
      "25854  UPDATE 2-US forces seize tanker carrying oil f...   \n",
      "25855  Norfolk-based destroyer to aid tanker seized b...   \n",
      "25856         US Navy SEALs board rogue Libya oil tanker   \n",
      "25857          U.S. forces seize control of rogue tanker   \n",
      "25858            US Navy SEALs seize fugitive oil tanker   \n",
      "25859  US Navy SEALs board tanker carrying oil from L...   \n",
      "25860  Shades of 'Captain Phillips': Navy SEALS retak...   \n",
      "25861      US Seals take control of rogue Libya oil ship   \n",
      "25862  US Navy SEALs seize stolen Libyan oil tanker h...   \n",
      "25863  Navy SEALs board and take control of oil tanke...   \n",
      "25864  US Special forces take control of rogue Libya ...   \n",
      "25865        Navy SEALs seize control of hijacked tanker   \n",
      "25866  US forces seize tanker carrying oil from Libya...   \n",
      "25867  US Navy Seals take control of rogue Libya oil ...   \n",
      "25868  US Seals take control of rogue Libya oil ship:...   \n",
      "25869  Navy SEALs board mystery tanker Morning Glory ...   \n",
      "25870  US Navy Seals retake hijacked oil tanker off t...   \n",
      "25871   Cyprus: SEALs take oil tanker from Libyan rebels   \n",
      "25872        US Navy Seals seize North Korean oil tanker   \n",
      "25873            Navy Seals board rogue Libya oil tanker   \n",
      "25874  US Navy Seal Commandos Seize North Korea Oil T...   \n",
      "25875  Israeli pair questioned over Pyongyang-bound o...   \n",
      "25876  Navy SEALs Board Oil Tanker Stolen From Libyan...   \n",
      "25877  US Seals take control of rogue Libya ship: Pen...   \n",
      "25878     Navy SEALS take control of hijacked oil tanker   \n",
      "25879  Navy SEALs take control of hijacked Libyan oil...   \n",
      "25880  U.S. Navy SEALs take control of North Korean-f...   \n",
      "25881  Oil tanker heading back to Libya after capture...   \n",
      "25882             US Seals storm 'oil theft' Libyan ship   \n",
      "\n",
      "                                                     URL  \\\n",
      "id                                                         \n",
      "1                                    100percentfedup.com   \n",
      "2                                    100percentfedup.com   \n",
      "3                                    100percentfedup.com   \n",
      "4                                    100percentfedup.com   \n",
      "5                                    100percentfedup.com   \n",
      "6                                    100percentfedup.com   \n",
      "7                                    100percentfedup.com   \n",
      "8                                    100percentfedup.com   \n",
      "9                                    100percentfedup.com   \n",
      "10                                   100percentfedup.com   \n",
      "11                                   100percentfedup.com   \n",
      "12                                   100percentfedup.com   \n",
      "13                                   100percentfedup.com   \n",
      "14                                   100percentfedup.com   \n",
      "15                                   100percentfedup.com   \n",
      "16                                   100percentfedup.com   \n",
      "17                                   100percentfedup.com   \n",
      "18                                   100percentfedup.com   \n",
      "19                                   100percentfedup.com   \n",
      "20                                   100percentfedup.com   \n",
      "21                                   100percentfedup.com   \n",
      "22                                   100percentfedup.com   \n",
      "23                                   100percentfedup.com   \n",
      "24                                   100percentfedup.com   \n",
      "25                                   100percentfedup.com   \n",
      "26                                   100percentfedup.com   \n",
      "27                                   100percentfedup.com   \n",
      "28                                   100percentfedup.com   \n",
      "29                                   100percentfedup.com   \n",
      "30                                   100percentfedup.com   \n",
      "...                                                  ...   \n",
      "25853  http://www.reuters.com/article/2014/03/17/usa-...   \n",
      "25854  http://www.reuters.com/article/2014/03/17/usa-...   \n",
      "25855  http://hamptonroads.com/2014/03/norfolkbased-d...   \n",
      "25856  http://www.aljazeera.com/news/africa/2014/03/u...   \n",
      "25857  http://www.ksdk.com/story/news/nation/2014/03/...   \n",
      "25858  http://www.latimes.com/world/worldnow/la-fg-wn...   \n",
      "25859  http://uk.reuters.com/article/2014/03/17/uk-us...   \n",
      "25860  http://www.arktimes.com/ArkansasBlog/archives/...   \n",
      "25861  http://www.ptinews.com/news/4509295_-US-Seals-...   \n",
      "25862  http://www.allvoices.com/contributed-news/1671...   \n",
      "25863  http://www.newser.com/article/a119b55666b54365...   \n",
      "25864  http://www.brisbanetimes.com.au/world/us-speci...   \n",
      "25865  http://www.abc17news.com/national-news/Navy-SE...   \n",
      "25866  http://www.worldbulletin.net/world/131244/us-f...   \n",
      "25867  http://news.yahoo.com/us-seals-control-rogue-l...   \n",
      "25868  http://www.ptinews.com/news/4509295_US-Seals-t...   \n",
      "25869  http://www.washingtonpost.com/news/morning-mix...   \n",
      "25870  http://www.rawstory.com/rs/2014/03/17/us-navy-...   \n",
      "25871  http://www.politico.com/story/2014/03/cyprus-n...   \n",
      "25872  http://voiceofrussia.com/news/2014_03_17/US-Na...   \n",
      "25873  http://blouinnews.com/78329/story/navy-seals-b...   \n",
      "25874  http://www.ibtimes.co.uk/us-navy-seal-commando...   \n",
      "25875  http://www.thestandard.com.hk/breaking_news_de...   \n",
      "25876  http://www.huffingtonpost.com/2014/03/17/navy-...   \n",
      "25877  http://www.ptinews.com/news/4509045_US-Seals-t...   \n",
      "25878  http://www.washingtontimes.com/news/2014/mar/1...   \n",
      "25879  http://wtkr.com/2014/03/17/navy-seals-take-con...   \n",
      "25880  http://www.dailymail.co.uk/news/article-258257...   \n",
      "25881  http://www.libyaherald.com/2014/03/17/oil-tank...   \n",
      "25882  http://www.upstreamonline.com/live/article1355...   \n",
      "\n",
      "       ratio_exclam_in_title  TARGET  \n",
      "id                                    \n",
      "1                   0.000000       1  \n",
      "2                   0.000000       1  \n",
      "3                   0.000000       1  \n",
      "4                   0.050000       1  \n",
      "5                   0.153846       1  \n",
      "6                   0.125000       1  \n",
      "7                   0.038462       1  \n",
      "8                   0.043478       1  \n",
      "9                   0.000000       1  \n",
      "10                  0.000000       1  \n",
      "11                  0.111111       1  \n",
      "12                  0.125000       1  \n",
      "13                  0.000000       1  \n",
      "14                  0.083333       1  \n",
      "15                  0.062500       1  \n",
      "16                  0.000000       1  \n",
      "17                  0.000000       1  \n",
      "18                  0.000000       1  \n",
      "19                  0.000000       1  \n",
      "20                  0.000000       1  \n",
      "21                  0.000000       1  \n",
      "22                  0.000000       1  \n",
      "23                  0.058824       1  \n",
      "24                  0.050000       1  \n",
      "25                  0.111111       1  \n",
      "26                  0.090909       1  \n",
      "27                  0.000000       1  \n",
      "28                  0.076923       1  \n",
      "29                  0.000000       1  \n",
      "30                  0.166667       1  \n",
      "...                      ...     ...  \n",
      "25853               0.000000       0  \n",
      "25854               0.000000       0  \n",
      "25855               0.000000       0  \n",
      "25856               0.000000       0  \n",
      "25857               0.000000       0  \n",
      "25858               0.000000       0  \n",
      "25859               0.000000       0  \n",
      "25860               0.000000       0  \n",
      "25861               0.000000       0  \n",
      "25862               0.000000       0  \n",
      "25863               0.000000       0  \n",
      "25864               0.000000       0  \n",
      "25865               0.000000       0  \n",
      "25866               0.000000       0  \n",
      "25867               0.000000       0  \n",
      "25868               0.000000       0  \n",
      "25869               0.000000       0  \n",
      "25870               0.000000       0  \n",
      "25871               0.000000       0  \n",
      "25872               0.000000       0  \n",
      "25873               0.000000       0  \n",
      "25874               0.000000       0  \n",
      "25875               0.000000       0  \n",
      "25876               0.000000       0  \n",
      "25877               0.000000       0  \n",
      "25878               0.000000       0  \n",
      "25879               0.000000       0  \n",
      "25880               0.000000       0  \n",
      "25881               0.000000       0  \n",
      "25882               0.000000       0  \n",
      "\n",
      "[25882 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# create new column, 'TARGET' with 1 fake and 0 for real\n",
    "sub_fake_df['TARGET'] = 1\n",
    "sub_fake_df = sub_fake_df.rename(columns = {'thread_title':'TITLE', 'site_url':'URL'})\n",
    "sub_real_df['TARGET'] = 0\n",
    "\n",
    "# combine the two dataframes\n",
    "combined_df = sub_fake_df.append(sub_real_df)\n",
    "\n",
    "# reorder the id index of the combined_df set\n",
    "# correct id labels\n",
    "combined_df['id'] = range(1, len(combined_df) + 1)\n",
    "combined_df = combined_df.set_index('id')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "combined_df = shuffle(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert combined_df into a new TAB DELIMITED csv\n",
    "combined_df.to_csv(\"cleaned_combined_dataset.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(combined_df)) < 0.75\n",
    "training_set = combined_df[sampler]\n",
    "test_set = combined_df[~sampler]\n",
    "\n",
    "#print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set.to_csv(\"training_set.csv\", sep='\\t', index=False)\n",
    "test_set.to_csv(\"holdout_set.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stem the \"fake news\" data\n",
    "ps = PorterStemmer()\n",
    "fake_blob = {} \n",
    "real_blob = {}\n",
    "for i in range(len(training_set['TARGET'])):\n",
    "    try:\n",
    "        ss8 = str(training_set['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    # if this is a 'fake' row entry\n",
    "    if training_set['TARGET'].iloc[i] == 1:\n",
    "        for stword in x:\n",
    "            if stword in fake_blob:\n",
    "                fake_blob[stword] = fake_blob[stword] + 1\n",
    "                #print(stword, \" \", fake_blob[stword])\n",
    "            else:\n",
    "                fake_blob.setdefault(stword, 1)\n",
    "                #print(stword,\" \", fake_blob[stword])\n",
    "                \n",
    "    # we found a 'real' row entry\n",
    "    else:\n",
    "        for stword in x:\n",
    "            if stword in real_blob:\n",
    "                real_blob[stword] = real_blob[stword] + 1\n",
    "            else:\n",
    "                real_blob.setdefault(stword, 1)\n",
    "# print(fakeBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # stem the \"real news\" data    \n",
    "# goodBlob = {}\n",
    "# for i in range(len(sub_real_df['TITLE'])):\n",
    "#     try:\n",
    "#         ss8 = str(sub_real_df['TITLE'].iloc[i].encode('utf8'))\n",
    "#     except:\n",
    "#         pass\n",
    "#     words = word_tokenize(ss8)\n",
    "#     x = set()\n",
    "#     for w in words:\n",
    "#         x.add(ps.stem(w).lower())\n",
    "\n",
    "#     for stword in x:\n",
    "#         if stword in goodBlob:\n",
    "#             goodBlob[stword] = goodBlob[stword] + 1\n",
    "#             #print(stword, \" \", goodBlob[stword])\n",
    "#         else:\n",
    "#             goodBlob.setdefault(stword,1)\n",
    "#             #print(stword,\" \", goodBlob[stword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskdict = {}\n",
    "for word in fake_blob:\n",
    "    if word in real_blob:\n",
    "        count = (fake_blob[word] + real_blob[word])\n",
    "    else:\n",
    "        count = fake_blob[word]\n",
    "    if count >= 10:\n",
    "        riskdict[word] = fake_blob[word] / count\n",
    "\n",
    "for word in real_blob:\n",
    "    if word not in fake_blob and real_blob[word] >= 10:\n",
    "        riskdict[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TITLE', 'URL', 'ratio_exclam_in_title', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']\n"
     ]
    }
   ],
   "source": [
    "# add four columns to the results_df: fakeaggregate, goodaggregate, riskword, safeword\n",
    "results_df = combined_df.copy()\n",
    "results_df['fake_aggregate'] = 0\n",
    "results_df['good_aggregate'] = 0\n",
    "results_df['risk_word'] = 0\n",
    "results_df['safe_word'] = 0\n",
    "\n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TITLE', 'URL', 'ratio_exclam_in_title', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']\n"
     ]
    }
   ],
   "source": [
    "riskword = \"\"\n",
    "for i in range(len(results_df['TITLE'])):\n",
    "    fakeaggregate = 0\n",
    "    goodaggregate = 0\n",
    "    riskyword = 0\n",
    "    safeword = 1\n",
    "    try:\n",
    "        ss8 = str(results_df['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    for stword in x:\n",
    "        if stword in riskdict:\n",
    "            if riskdict[stword] > riskyword:\n",
    "                riskword = riskdict[stword]\n",
    "            if riskdict[stword] < safeword:\n",
    "                safeword = riskdict[stword]\n",
    "\n",
    "        if stword in fake_blob:\n",
    "            fakeaggregate = fake_blob[stword] + fakeaggregate\n",
    "\n",
    "        if stword in real_blob:\n",
    "            goodaggregate = real_blob[stword] + goodaggregate\n",
    "    # update the results to results_df, training_df, test_df        \n",
    "    results_df.set_value(i, 'fake_aggregate', fakeaggregate)\n",
    "    results_df.set_value(i, 'good_aggregate', goodaggregate)\n",
    "    results_df.set_value(i, 'risk_word', riskword)\n",
    "    results_df.set_value(i, 'safe_word', safeword)\n",
    "    \n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TARGET  ratio_exclam_in_title  fake_aggregate  good_aggregate  \\\n",
      "id                                                                     \n",
      "18902     0.0               0.000000          9334.0          8483.0   \n",
      "6199      1.0               0.000000         13420.0         11418.0   \n",
      "25056     0.0               0.000000         13193.0         11461.0   \n",
      "1402      1.0               0.000000         12655.0          9865.0   \n",
      "12862     1.0               0.000000         14408.0         13514.0   \n",
      "5170      1.0               0.000000         11023.0          9637.0   \n",
      "15680     0.0               0.000000         14489.0         14328.0   \n",
      "12334     1.0               0.000000          9700.0          8794.0   \n",
      "8652      1.0               0.000000         12061.0          9550.0   \n",
      "23033     0.0               0.000000         15486.0         12330.0   \n",
      "6333      1.0               0.000000         15484.0         13054.0   \n",
      "9106      1.0               0.000000         14212.0         17755.0   \n",
      "11100     1.0               0.000000         13645.0         17026.0   \n",
      "13132     0.0               0.000000         10938.0         10220.0   \n",
      "1535      1.0               0.000000         14320.0         10806.0   \n",
      "23185     0.0               0.000000         12696.0         18008.0   \n",
      "17472     0.0               0.000000          9922.0          9092.0   \n",
      "9613      1.0               0.000000          9409.0          8343.0   \n",
      "3329      1.0               0.125000          9396.0          8482.0   \n",
      "24673     0.0               0.000000          6057.0         11929.0   \n",
      "23315     0.0               0.000000         12328.0         10471.0   \n",
      "8845      1.0               0.000000         11031.0          9995.0   \n",
      "4794      1.0               0.000000          6282.0         12226.0   \n",
      "4177      1.0               0.000000         11260.0          9512.0   \n",
      "25074     0.0               0.000000         17868.0         18972.0   \n",
      "17742     0.0               0.000000         15729.0         13361.0   \n",
      "12947     0.0               0.000000         16582.0         12601.0   \n",
      "13247     0.0               0.000000         13820.0         10987.0   \n",
      "19448     0.0               0.000000         11212.0         10099.0   \n",
      "12698     1.0               0.000000         16110.0         13479.0   \n",
      "...       ...                    ...             ...             ...   \n",
      "1443      1.0               0.000000         12002.0         10494.0   \n",
      "21301     0.0               0.000000         14119.0         11377.0   \n",
      "13472     0.0               0.000000          3678.0         10011.0   \n",
      "19966     0.0               0.000000          9339.0          8084.0   \n",
      "11567     1.0               0.000000         15460.0         12786.0   \n",
      "17460     0.0               0.000000         12225.0          9795.0   \n",
      "10001     1.0               0.000000         13920.0         10735.0   \n",
      "2378      1.0               0.000000         11341.0          9440.0   \n",
      "25284     0.0               0.000000         16686.0         13990.0   \n",
      "21682     0.0               0.000000          3594.0          9826.0   \n",
      "24440     0.0               0.000000         10470.0          9745.0   \n",
      "1919      1.0               0.000000         18473.0         19815.0   \n",
      "13899     0.0               0.000000         13723.0         11250.0   \n",
      "16130     0.0               0.111111         14309.0         12452.0   \n",
      "2943      1.0               0.000000         14503.0         18687.0   \n",
      "6330      1.0               0.000000         15389.0         11543.0   \n",
      "14436     0.0               0.000000         17554.0         12840.0   \n",
      "10891     1.0               0.000000         14607.0         12292.0   \n",
      "17671     0.0               0.000000         11381.0         11687.0   \n",
      "18608     0.0               0.000000         13011.0         10882.0   \n",
      "11893     1.0               0.000000         10616.0          9216.0   \n",
      "6622      1.0               0.000000         16223.0         13035.0   \n",
      "13698     0.0               0.000000         11846.0         10928.0   \n",
      "6198      1.0               0.000000         12993.0         10664.0   \n",
      "12572     1.0               0.000000          9788.0          8865.0   \n",
      "153       1.0               0.428571         10568.0          9572.0   \n",
      "9476      1.0               0.000000         18757.0         14063.0   \n",
      "25196     0.0               0.000000         15269.0         17775.0   \n",
      "12674     1.0               0.000000         15617.0         13457.0   \n",
      "0         NaN                    NaN         12774.0         16305.0   \n",
      "\n",
      "       risk_word  safe_word  \n",
      "id                           \n",
      "18902   0.066667   0.057971  \n",
      "6199    0.333333   0.000000  \n",
      "25056   1.000000   0.458552  \n",
      "1402    0.741497   0.000000  \n",
      "12862   0.541667   0.000000  \n",
      "5170    0.547000   0.311258  \n",
      "15680   0.492230   0.043384  \n",
      "12334   0.118644   0.000000  \n",
      "8652    0.500000   0.428571  \n",
      "23033   0.500000   0.438596  \n",
      "6333    0.749004   0.181818  \n",
      "9106    0.357143   0.000000  \n",
      "11100   0.603774   0.173913  \n",
      "13132   0.357143   0.000000  \n",
      "1535    1.000000   0.448276  \n",
      "23185   0.716667   0.018519  \n",
      "17472   0.500442   0.000000  \n",
      "9613    0.294574   0.185185  \n",
      "3329    0.472050   0.000000  \n",
      "24673   0.043384   0.043384  \n",
      "23315   0.547000   0.343750  \n",
      "8845    0.492230   0.000000  \n",
      "4794    0.513410   0.000000  \n",
      "4177    0.602804   0.485251  \n",
      "25074   0.250000   0.000000  \n",
      "17742   0.736842   0.088235  \n",
      "12947   1.000000   0.472272  \n",
      "13247   0.733333   0.244604  \n",
      "19448   0.422819   0.144737  \n",
      "12698   0.749004   0.403846  \n",
      "...          ...        ...  \n",
      "1443    0.547000   0.000000  \n",
      "21301   0.842105   0.425434  \n",
      "13472   0.285714   0.000000  \n",
      "19966   0.534332   0.534332  \n",
      "11567   0.534332   0.458552  \n",
      "17460   0.246377   0.000000  \n",
      "10001   0.644444   0.150000  \n",
      "2378    0.773585   0.534332  \n",
      "25284   0.606061   0.265306  \n",
      "21682   0.264706   0.007407  \n",
      "24440   0.534332   0.000000  \n",
      "1919    0.656250   0.195715  \n",
      "13899   0.691964   0.534332  \n",
      "16130   0.547000   0.133333  \n",
      "2943    0.045455   0.045455  \n",
      "6330    1.000000   0.458552  \n",
      "14436   0.921569   0.534332  \n",
      "10891   0.529412   0.000000  \n",
      "17671   0.772727   0.000000  \n",
      "18608   1.000000   0.534332  \n",
      "11893   0.573529   0.379310  \n",
      "6622    0.547325   0.000000  \n",
      "13698   0.100000   0.000000  \n",
      "6198    0.505051   0.372549  \n",
      "12572   0.173913   0.022523  \n",
      "153     0.606838   0.275862  \n",
      "9476    1.000000   0.485251  \n",
      "25196   0.547000   0.018519  \n",
      "12674   0.547000   0.165289  \n",
      "0       0.534332   0.000000  \n",
      "\n",
      "[19356 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(results_df)) < 0.75\n",
    "new_training = results_df[sampler]\n",
    "new_test = results_df[~sampler]\n",
    "\n",
    "# new_training = new_training[['TITLE', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "new_training = new_training[['TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "print(new_training)\n",
    "\n",
    "holdout_title_features = new_test[['TITLE', 'TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "\n",
    "new_training.to_csv(\"new_training.csv\", sep='\\t', index=False)\n",
    "new_test.to_csv(\"new_holdout.csv\", sep='\\t', index=False)\n",
    "\n",
    "holdout_title_features.to_csv(\"holdout_title_features.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
