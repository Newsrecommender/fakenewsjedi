{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "\n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n",
      "(422419, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import rcParams\n",
    "\n",
    "fake_df = pd.DataFrame.from_csv(\"fake.csv\")\n",
    "real_df = pd.DataFrame.from_csv(\"../uci-news-aggregator.csv\")\n",
    "\n",
    "fake_num_rows = fake_df.shape\n",
    "print(fake_num_rows)\n",
    "\n",
    "real_num_rows = real_df.shape\n",
    "print(real_num_rows)\n",
    "\n",
    "# df.head(100)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_df = real_df.head(12999)\n",
    "# print(real_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts_by_type = fake_df['type'].value_counts()\n",
    "print(counts_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['main_img_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(fake_df['spam_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['site_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_of_spam = fake_df['spam_score'].value_counts()\n",
    "# print(counts_of_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = fake_df.columns.tolist()\n",
    "# print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_replies = fake_df['replies_count'].value_counts()\n",
    "# print(counts_by_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_author = fake_df['author'].value_counts()\n",
    "# print(counts_by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_domain_rank = fake_df['domain_rank'].value_counts()\n",
    "# print(counts_by_domain_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "# \n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n"
     ]
    }
   ],
   "source": [
    "# create new \"id\" column in df \n",
    "# reorder column names, setting \"id\" as first column and delete \"uuid\" col \n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "fake_df = fake_df[['site_url', 'domain_rank', 'author', 'published', 'title', 'thread_title', 'text', 'ord_in_thread', 'crawled', 'country', 'language', 'spam_score', 'main_img_url', 'replies_count', 'participants_count', 'likes', 'comments', 'shares', 'type']]\n",
    "print(fake_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract text body from fake and save to file\n",
    "fake_text_only = fake_df[['text']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(fake_text_only) + 1):\n",
    "    text = fake_text_only.loc[i, 'text']\n",
    "    if type(text) != float:\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        text = text.split(\"\\t\")\n",
    "        text = \" \".join(text)\n",
    "        fake_text_only.set_value(i, 'text', text)\n",
    "\n",
    "fake_text_only.to_csv(\"fake_body_only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count ratio of number of exclamation marks to words in the given string\n",
    "def count_ratio_exclams(string):\n",
    "    exclam = '!'\n",
    "    space = \" \"\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_spaces = string.count(space)\n",
    "    if num_spaces == 0:\n",
    "        return num_exclams\n",
    "    else:\n",
    "        return num_exclams / num_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the ratio of exclams to question marks + periods in the given string\n",
    "def exclam_ratio_text_body(string):\n",
    "    exclam = '!'\n",
    "    period = '.'\n",
    "    question = '?'\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_period = string.count(period)\n",
    "    num_question = string.count(question)\n",
    "    if num_period + num_question == 0:\n",
    "        return num_exclams\n",
    "    return num_exclams / (num_period + num_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclam_in_title\n",
    "fake_df.assign(ratio_exclam_in_title=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "\n",
    "fake_df = fake_df[fake_df['thread_title'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    thread_title = fake_df.loc[i, 'thread_title']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "# counts_by_title_exclams = fake_df.total_exclam_in_title.value_counts()\n",
    "# print(counts_by_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for total_exclam_in_text count\n",
    "fake_df.assign(ratio_exclam_in_text_body=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "fake_df = fake_df[fake_df['text'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_text_body', count)\n",
    "\n",
    "ratio_in_text_body = fake_df.ratio_exclam_in_text_body.value_counts()\n",
    "# print(ratio_in_text_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclams_in_text count\n",
    "fake_df.assign(ratio_exclams_in_text=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclams_in_text\"\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclams_in_text_body', count)\n",
    "\n",
    "counts_ratio_exclams = fake_df.ratio_exclams_in_text.value_counts()\n",
    "# print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclam_in_title count\n",
    "real_df.assign(ratio_exclam_in_title=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclam_in_title\"\n",
    "for i in range(1, len(real_df) + 1):\n",
    "    thread_title = real_df.loc[i, 'TITLE']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    real_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "counts_ratio_exclams = real_df.ratio_exclam_in_title.value_counts()\n",
    "print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create series of total exclamation counts in each row's title\n",
    "# for index, row in df.iterrows():\n",
    "#     count = count_total_exclamation(row.title)\n",
    "#     print(count)\n",
    "#     df.loc[:,'total_crime'] = df.apply(get_total_crime, axis=1)\n",
    "#     df.loc[index, row.total_exclam_in_title] = count\n",
    "\n",
    "\n",
    "# df.loc[:, 'total_exclam_in_title'] = df.apply(count_total_exclams, axis=1)    \n",
    "# count_title_exclams = df['total_exclam_in_title'].value_counts()\n",
    "# print(count_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make  copy of the fake_df containing only the thread_title & site_url\n",
    "sub_fake_df = fake_df[['thread_title', 'site_url', 'ratio_exclam_in_title']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    title = sub_fake_df.loc[i, 'thread_title']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_fake_df.set_value(i, 'thread_title', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    url = sub_fake_df.loc[i, 'site_url']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_fake_df.set_value(i, 'site_url', url)\n",
    "\n",
    "# turn all tabs into spaces\n",
    "# x = \"The bananas are yellow and green\"\n",
    "# x = x.split(\" \")\n",
    "# print(x)\n",
    "# x = \"+\".join(x)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_real_df = real_df[['TITLE', 'URL', 'ratio_exclam_in_title']].copy()\n",
    "sub_real_df = sub_real_df.head(12941)\n",
    "\n",
    "# correct id labels\n",
    "sub_real_df['id'] = range(1, len(sub_real_df) + 1)\n",
    "sub_real_df = sub_real_df.set_index('id')\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    title = sub_real_df.loc[i, 'TITLE']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_real_df.set_value(i, 'TITLE', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    url = sub_real_df.loc[i, 'URL']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_real_df.set_value(i, 'URL', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new column, 'TARGET' with 1 fake and 0 for real\n",
    "sub_fake_df['TARGET'] = 1\n",
    "sub_fake_df = sub_fake_df.rename(columns = {'thread_title':'TITLE', 'site_url':'URL'})\n",
    "sub_real_df['TARGET'] = 0\n",
    "\n",
    "# combine the two dataframes\n",
    "combined_df = sub_fake_df.append(sub_real_df)\n",
    "\n",
    "# reorder the id index of the combined_df set\n",
    "# correct id labels\n",
    "combined_df['id'] = range(1, len(combined_df) + 1)\n",
    "combined_df = combined_df.set_index('id')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "combined_df = shuffle(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert combined_df into a new TAB DELIMITED csv\n",
    "combined_df.to_csv(\"cleaned_combined_dataset.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(combined_df)) < 0.75\n",
    "training_set = combined_df[sampler]\n",
    "test_set = combined_df[~sampler]\n",
    "\n",
    "#print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set.to_csv(\"training_set.csv\", sep='\\t', index=False)\n",
    "test_set.to_csv(\"holdout_set.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stem the \"fake news\" data\n",
    "ps = PorterStemmer()\n",
    "fake_blob = {} \n",
    "real_blob = {}\n",
    "for i in range(len(training_set['TARGET'])):\n",
    "    try:\n",
    "        ss8 = str(training_set['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    # if this is a 'fake' row entry\n",
    "    if training_set['TARGET'].iloc[i] == 1:\n",
    "        for stword in x:\n",
    "            if stword in fake_blob:\n",
    "                fake_blob[stword] = fake_blob[stword] + 1\n",
    "                #print(stword, \" \", fake_blob[stword])\n",
    "            else:\n",
    "                fake_blob.setdefault(stword, 1)\n",
    "                #print(stword,\" \", fake_blob[stword])\n",
    "                \n",
    "    # we found a 'real' row entry\n",
    "    else:\n",
    "        for stword in x:\n",
    "            if stword in real_blob:\n",
    "                real_blob[stword] = real_blob[stword] + 1\n",
    "            else:\n",
    "                real_blob.setdefault(stword, 1)\n",
    "# print(fakeBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # stem the \"real news\" data    \n",
    "# goodBlob = {}\n",
    "# for i in range(len(sub_real_df['TITLE'])):\n",
    "#     try:\n",
    "#         ss8 = str(sub_real_df['TITLE'].iloc[i].encode('utf8'))\n",
    "#     except:\n",
    "#         pass\n",
    "#     words = word_tokenize(ss8)\n",
    "#     x = set()\n",
    "#     for w in words:\n",
    "#         x.add(ps.stem(w).lower())\n",
    "\n",
    "#     for stword in x:\n",
    "#         if stword in goodBlob:\n",
    "#             goodBlob[stword] = goodBlob[stword] + 1\n",
    "#             #print(stword, \" \", goodBlob[stword])\n",
    "#         else:\n",
    "#             goodBlob.setdefault(stword,1)\n",
    "#             #print(stword,\" \", goodBlob[stword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskdict = {}\n",
    "for word in fake_blob:\n",
    "    if word in real_blob:\n",
    "        count = (fake_blob[word] + real_blob[word])\n",
    "    else:\n",
    "        count = fake_blob[word]\n",
    "    if count >= 10:\n",
    "        riskdict[word] = fake_blob[word] / count\n",
    "\n",
    "for word in real_blob:\n",
    "    if word not in fake_blob and real_blob[word] >= 10:\n",
    "        riskdict[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add four columns to the results_df: fakeaggregate, goodaggregate, riskword, safeword\n",
    "results_df = combined_df.copy()\n",
    "results_df['fake_aggregate'] = 0\n",
    "results_df['good_aggregate'] = 0\n",
    "results_df['risk_word'] = 0\n",
    "results_df['safe_word'] = 0\n",
    "\n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskword = \"\"\n",
    "for i in range(len(results_df['TITLE'])):\n",
    "    fakeaggregate = 0\n",
    "    goodaggregate = 0\n",
    "    riskyword = 0\n",
    "    safeword = 1\n",
    "    try:\n",
    "        ss8 = str(results_df['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    for stword in x:\n",
    "        if stword in riskdict:\n",
    "            if riskdict[stword] > riskyword:\n",
    "                riskword = riskdict[stword]\n",
    "            if riskdict[stword] < safeword:\n",
    "                safeword = riskdict[stword]\n",
    "\n",
    "        if stword in fake_blob:\n",
    "            fakeaggregate = fake_blob[stword] + fakeaggregate\n",
    "\n",
    "        if stword in real_blob:\n",
    "            goodaggregate = real_blob[stword] + goodaggregate\n",
    "    # update the results to results_df, training_df, test_df        \n",
    "    results_df.set_value(i, 'fake_aggregate', fakeaggregate)\n",
    "    results_df.set_value(i, 'good_aggregate', goodaggregate)\n",
    "    results_df.set_value(i, 'risk_word', riskword)\n",
    "    results_df.set_value(i, 'safe_word', safeword)\n",
    "    \n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(results_df)) < 0.75\n",
    "new_training = results_df[sampler]\n",
    "new_test = results_df[~sampler]\n",
    "\n",
    "# new_training = new_training[['TITLE', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "new_training = new_training[['TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "print(new_training)\n",
    "\n",
    "holdout_title_features = new_test[['TITLE', 'TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "\n",
    "new_training.to_csv(\"new_training.csv\", sep='\\t', index=False)\n",
    "new_test.to_csv(\"new_holdout.csv\", sep='\\t', index=False)\n",
    "\n",
    "holdout_title_features.to_csv(\"holdout_title_features.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TARGET  Prediction  Unnamed: 3  Unnamed: 4     0     0.1    0.2  \\\n",
      "row_id                                                                    \n",
      "0            0    0.001520         NaN         NaN  0.02  2028.0    3.0   \n",
      "1            0    0.103507         NaN         NaN  0.04  2435.0    4.0   \n",
      "2            0    0.002745         NaN         NaN  0.06  2655.0    8.0   \n",
      "3            1    0.998342         NaN         NaN  0.08  2782.0   10.0   \n",
      "4            0    0.012193         NaN         NaN  0.10  2868.0   11.0   \n",
      "5            0    0.022689         NaN         NaN  0.12  2922.0   14.0   \n",
      "6            0    0.025135         NaN         NaN  0.14  2971.0   16.0   \n",
      "7            1    0.936227         NaN         NaN  0.16  3006.0   20.0   \n",
      "8            1    0.998647         NaN         NaN  0.18  3038.0   22.0   \n",
      "9            1    0.664123         NaN         NaN  0.20  3062.0   28.0   \n",
      "10           0    0.027565         NaN         NaN  0.22  3084.0   33.0   \n",
      "11           0    0.002182         NaN         NaN  0.24  3102.0   34.0   \n",
      "12           1    0.998274         NaN         NaN  0.26  3117.0   38.0   \n",
      "13           1    0.995495         NaN         NaN  0.28  3127.0   40.0   \n",
      "14           0    0.046371         NaN         NaN  0.30  3139.0   45.0   \n",
      "15           1    0.957660         NaN         NaN  0.32  3151.0   50.0   \n",
      "16           1    0.460874         NaN         NaN  0.34  3159.0   50.0   \n",
      "17           1    0.950155         NaN         NaN  0.36  3168.0   53.0   \n",
      "18           1    0.991417         NaN         NaN  0.38  3175.0   57.0   \n",
      "19           1    0.975578         NaN         NaN  0.40  3181.0   60.0   \n",
      "20           0    0.002091         NaN         NaN  0.42  3186.0   63.0   \n",
      "21           1    0.976225         NaN         NaN  0.44  3193.0   69.0   \n",
      "22           0    0.026051         NaN         NaN  0.46  3199.0   76.0   \n",
      "23           1    0.998478         NaN         NaN  0.48  3202.0   81.0   \n",
      "24           0    0.032179         NaN         NaN  0.50  3208.0   87.0   \n",
      "25           1    0.983795         NaN         NaN  0.52  3209.0   92.0   \n",
      "26           1    0.951778         NaN         NaN  0.54  3211.0   97.0   \n",
      "27           0    0.008969         NaN         NaN  0.56  3213.0  104.0   \n",
      "28           1    0.966822         NaN         NaN  0.58  3220.0  107.0   \n",
      "29           1    0.986244         NaN         NaN  0.60  3225.0  116.0   \n",
      "...        ...         ...         ...         ...   ...     ...    ...   \n",
      "6497         0    0.003881         NaN         NaN   NaN     NaN    NaN   \n",
      "6498         1    0.981095         NaN         NaN   NaN     NaN    NaN   \n",
      "6499         1    0.922345         NaN         NaN   NaN     NaN    NaN   \n",
      "6500         1    0.998981         NaN         NaN   NaN     NaN    NaN   \n",
      "6501         1    0.947971         NaN         NaN   NaN     NaN    NaN   \n",
      "6502         1    0.991340         NaN         NaN   NaN     NaN    NaN   \n",
      "6503         0    0.011372         NaN         NaN   NaN     NaN    NaN   \n",
      "6504         1    0.855106         NaN         NaN   NaN     NaN    NaN   \n",
      "6505         0    0.001118         NaN         NaN   NaN     NaN    NaN   \n",
      "6506         1    0.859763         NaN         NaN   NaN     NaN    NaN   \n",
      "6507         1    0.978019         NaN         NaN   NaN     NaN    NaN   \n",
      "6508         0    0.012459         NaN         NaN   NaN     NaN    NaN   \n",
      "6509         1    0.929133         NaN         NaN   NaN     NaN    NaN   \n",
      "6510         1    0.964745         NaN         NaN   NaN     NaN    NaN   \n",
      "6511         0    0.142710         NaN         NaN   NaN     NaN    NaN   \n",
      "6512         0    0.023325         NaN         NaN   NaN     NaN    NaN   \n",
      "6513         1    0.971977         NaN         NaN   NaN     NaN    NaN   \n",
      "6514         1    0.855282         NaN         NaN   NaN     NaN    NaN   \n",
      "6515         0    0.001906         NaN         NaN   NaN     NaN    NaN   \n",
      "6516         0    0.001033         NaN         NaN   NaN     NaN    NaN   \n",
      "6517         1    0.994649         NaN         NaN   NaN     NaN    NaN   \n",
      "6518         0    0.006456         NaN         NaN   NaN     NaN    NaN   \n",
      "6519         1    0.998963         NaN         NaN   NaN     NaN    NaN   \n",
      "6520         1    0.998955         NaN         NaN   NaN     NaN    NaN   \n",
      "6521         0    0.033259         NaN         NaN   NaN     NaN    NaN   \n",
      "6522         1    0.983095         NaN         NaN   NaN     NaN    NaN   \n",
      "6523         1    0.997107         NaN         NaN   NaN     NaN    NaN   \n",
      "6524         1    0.997032         NaN         NaN   NaN     NaN    NaN   \n",
      "6525         1    0.315211         NaN         NaN   NaN     NaN    NaN   \n",
      "6526         1    0.938806         NaN         NaN   NaN     NaN    NaN   \n",
      "\n",
      "         0.00% 0.00%.1  0.00%.2  \n",
      "row_id                           \n",
      "0       62.13%   0.09%  -62.04%  \n",
      "1       74.60%   0.12%  -74.48%  \n",
      "2       81.34%   0.25%  -81.10%  \n",
      "3       85.23%   0.31%  -84.93%  \n",
      "4       87.87%   0.34%  -87.53%  \n",
      "5       89.52%   0.43%  -89.09%  \n",
      "6       91.02%   0.49%  -90.53%  \n",
      "7       92.10%   0.61%  -91.48%  \n",
      "8       93.08%   0.67%  -92.40%  \n",
      "9       93.81%   0.86%  -92.95%  \n",
      "10      94.49%   1.01%  -93.47%  \n",
      "11      95.04%   1.04%  -93.99%  \n",
      "12      95.50%   1.16%  -94.33%  \n",
      "13      95.80%   1.23%  -94.58%  \n",
      "14      96.17%   1.38%  -94.79%  \n",
      "15      96.54%   1.53%  -95.01%  \n",
      "16      96.78%   1.53%  -95.25%  \n",
      "17      97.06%   1.62%  -95.43%  \n",
      "18      97.27%   1.75%  -95.53%  \n",
      "19      97.46%   1.84%  -95.62%  \n",
      "20      97.61%   1.93%  -95.68%  \n",
      "21      97.82%   2.11%  -95.71%  \n",
      "22      98.01%   2.33%  -95.68%  \n",
      "23      98.10%   2.48%  -95.62%  \n",
      "24      98.28%   2.67%  -95.62%  \n",
      "25      98.31%   2.82%  -95.50%  \n",
      "26      98.38%   2.97%  -95.40%  \n",
      "27      98.44%   3.19%  -95.25%  \n",
      "28      98.65%   3.28%  -95.37%  \n",
      "29      98.81%   3.56%  -95.25%  \n",
      "...        ...     ...      ...  \n",
      "6497       NaN     NaN      NaN  \n",
      "6498       NaN     NaN      NaN  \n",
      "6499       NaN     NaN      NaN  \n",
      "6500       NaN     NaN      NaN  \n",
      "6501       NaN     NaN      NaN  \n",
      "6502       NaN     NaN      NaN  \n",
      "6503       NaN     NaN      NaN  \n",
      "6504       NaN     NaN      NaN  \n",
      "6505       NaN     NaN      NaN  \n",
      "6506       NaN     NaN      NaN  \n",
      "6507       NaN     NaN      NaN  \n",
      "6508       NaN     NaN      NaN  \n",
      "6509       NaN     NaN      NaN  \n",
      "6510       NaN     NaN      NaN  \n",
      "6511       NaN     NaN      NaN  \n",
      "6512       NaN     NaN      NaN  \n",
      "6513       NaN     NaN      NaN  \n",
      "6514       NaN     NaN      NaN  \n",
      "6515       NaN     NaN      NaN  \n",
      "6516       NaN     NaN      NaN  \n",
      "6517       NaN     NaN      NaN  \n",
      "6518       NaN     NaN      NaN  \n",
      "6519       NaN     NaN      NaN  \n",
      "6520       NaN     NaN      NaN  \n",
      "6521       NaN     NaN      NaN  \n",
      "6522       NaN     NaN      NaN  \n",
      "6523       NaN     NaN      NaN  \n",
      "6524       NaN     NaN      NaN  \n",
      "6525       NaN     NaN      NaN  \n",
      "6526       NaN     NaN      NaN  \n",
      "\n",
      "[6527 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Kolmogorov-Smirnoff Test:\n",
    "\n",
    "prediction_df = pd.DataFrame.from_csv(\"good_model_prediction_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
